{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from typing import Tuple\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import json\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "dataset = pd.read_csv('data/multisession-eeg.csv')\n",
    "fromstring = lambda array_str: np.fromstring(array_str, dtype=float, sep=',')\n",
    "dataset.raw_fft = dataset.raw_fft.apply(fromstring)\n",
    "# dataset.raw_fft.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passthoughts\n",
    "\n",
    "What if you could simply *think your password*? That's the premise behind *passthoughts*. We'll lay this out as a classification problem:\n",
    "\n",
    "> Given a reading, and a person, is that person who they claim to be?\n",
    "\n",
    "We'll structure this problem as follows: For each subject, we'll train a classifier. That subject's readings will be positive example, and everyone else's readings will be negative examples.\n",
    "\n",
    "We can make this a little fancier by having people use specific thoughts (e.g. \"focus on your breathing,\" \"sing a song in your head,\" etc). We'll make sure our methods can handle this case, but for the time being, we'll just use the `\"unabeled\"` readings - people doing nothing in particular.\n",
    "\n",
    "We'll use subject `A` as our \"target\" individual. We will train on this subject, and train against the other subjects in the corpus (subjects `B` and `C`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_matrix (series):\n",
    "    return np.array([ x for x in series ])\n",
    "\n",
    "def readings_right_subject_right_task (subj, task, session=0):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] == subj) &\n",
    "        (dataset['label'] == task) &\n",
    "        (dataset['session'] == session)\n",
    "    ].raw_fft)\n",
    "\n",
    "def readings_wrong_subj_any_task (subj):\n",
    "    return to_matrix(dataset[\n",
    "        (dataset['subject'] != subj)\n",
    "    ].raw_fft)\n",
    "\n",
    "\n",
    "def readings_wrong_subj_any_task_or_right_subj_wrong_task (subj, task):\n",
    "    a = to_matrix(dataset[ \n",
    "        (dataset['subject'] == subj) & (dataset['label'] != task)\n",
    "    ].raw_fft)\n",
    "    b = readings_wrong_subj_any_task (subj)\n",
    "    return np.concatenate((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 516), (40, 516))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = readings_right_subject_right_task('A', 'unlabeled', 0)\n",
    "negative = readings_wrong_subj_any_task('A')\n",
    "positive.shape, positive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we structured our positive and negative examples:\n",
    "\n",
    "- *Positive examples*: The right person thinking the right task.\n",
    "\n",
    "- *Negative examples*: The wrong person thinking any task (whether it is right or wrong).\n",
    "\n",
    "In the context of passthoughts, we could consider other possibilites for selecting positive and negative features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For instance, we could also structure them in the following way:\n",
    "- positive examples: The right person thinking the right task.\n",
    "- negative examples: The wrong person thinking any task (whether it is right or wrong) or the right person thinking the wrong task.\n",
    "\n",
    "Possible consequences of this setup, pros and cons:  \n",
    "- This configuration penalizes for not doing the right task even though being the correct person. That is, the inherence factor is taken to be more important in this configuration than the knowledge factor when classifying. In the original configuration, the inherence factor and the knowledge factor are equally important. This can be seen as a good configuration, if we want to train the classifier on a thinking style of the person (of a \"how the person thinks\") instead of \"what a person thinks\". This would mean, however, that we assume, that something like an individual thinking style exists.\n",
    "- The disadvantage to this is that there is non-stationarity, which might lead to a higher FRR (False Rejection Rate) for the right subject over time.\n",
    "- Also, since we place so much emphasis on the thinking style in this setting, the assumption of the individual thinking style should be correct. We can test for this: would we get a higher FAR with my configuration? This is indeed the case (see below). That means, the classifier cannot distinguish so well between the right subject for different (correct and incorrect) tasks. So the inherence factor is very important in the correct classification, maybe even so, as to stipulate an inherent \"thinking style\".\n",
    "\n",
    "We might evaluate this selection with the function \"readings_wrong_subj_any_task_or_right_subj_wrong_task\" above, which makes the set of negative examples bigger. I will do that below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll turn these data into our feature/label matrices `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([positive, negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([ 0 for x in positive] + [ 1 for x in negative])\n",
    "assert X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are assigning `0` to \"positive\" examples, and `1` to \"negative\" examples. That means `0` will mean \"ACCEPT\" and `1` will mean \"REJECT.\"\n",
    "\n",
    "Now, let's train and test a classifier! And estimate the classifier's accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fresh_clf () -> XGBClassifier:\n",
    "    return XGBClassifier(\n",
    "        objective= 'binary:logistic',\n",
    "        seed=27)\n",
    "\n",
    "def xgb_cross_validate (\n",
    "    X: np.array,\n",
    "    y: np.array,\n",
    "    nfold: int=7\n",
    ") -> Tuple[XGBClassifier, pd.DataFrame]:\n",
    "    # eval_metrics:\n",
    "    # http://xgboost.readthedocs.io/en/latest//parameter.html\n",
    "    metrics = ['error@0.1', 'auc']\n",
    "#     metrics = [ 'auc' ]\n",
    "    # we use the @ syntax to override the default of 0.5 as the threshold for 0 / 1 classification\n",
    "    # the intent here to to minimize FAR at the expense of FRR\n",
    "    alg = fresh_clf()\n",
    "    xgtrain = xgb.DMatrix(X,y)\n",
    "    param = alg.get_xgb_params()\n",
    "    cvresults = xgb.cv(param,\n",
    "                      xgtrain,\n",
    "                      num_boost_round=alg.get_params()['n_estimators'],\n",
    "                      nfold=nfold,\n",
    "                      metrics=metrics,\n",
    "                      early_stopping_rounds=100\n",
    "                      )\n",
    "    alg.set_params(n_estimators=cvresults.shape[0])\n",
    "    alg.fit(X,y,eval_metric=metrics)\n",
    "    return alg, cvresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = sklearn.model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "clf, cvres = xgb_cross_validate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98329355608591884"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For authentication, what we want even more than \"accuracy\". Here are two metrics:\n",
    "\n",
    "- False Acceptance Rate (FAR): The percentage of readings *not* from subject A incorrectly classified \"ACCEPT.\"\n",
    "- False Rejection Rate (FRR): The percentage of readings *from* subject A incorrectly classified 'REJECT.\"\n",
    "\n",
    "For authentication /security/, we want FAR to be as low as possible (so nobody can break in).\n",
    "For authentication /usability/, we want FRR to be low (so user's don't get frustrated constantly re-trying their passthought)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def far_frr (classifier, features, labels):\n",
    "    # predict all the labels\n",
    "    y_pred = classifier.predict(features)\n",
    "    false_accepts = 0\n",
    "    false_rejects = 0\n",
    "    for predicted, actual in zip(y_pred, labels):\n",
    "        # if we should have rejected,\n",
    "        # but in fact accepted,\n",
    "        if (actual == 1) and (predicted == 0):\n",
    "            # increment false accepts\n",
    "            false_accepts += 1\n",
    "        # if we should have accepted,\n",
    "        # but in fact rejected,\n",
    "        if (actual == 0) and (predicted == 1):\n",
    "            # increment false rejections\n",
    "            false_rejects += 1\n",
    "    # calculate proportions for each\n",
    "    far = false_accepts / len(list(filter(lambda x: x == 0, y_pred)))\n",
    "    frr = false_rejects / len(list(filter(lambda x: x == 1, y_pred)))\n",
    "    return far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 30.0% - FRR: 0.9779951100244498%'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_validate, y_validate)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 516), (40, 516))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive2 = readings_right_subject_right_task('A', 'unlabeled', 0)\n",
    "negative2 = readings_wrong_subj_any_task_or_right_subj_wrong_task('A', 'unlabeled')\n",
    "positive2.shape, positive2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2 = np.concatenate([positive2, negative2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2 = np.array([ 0 for x in positive2] + [ 1 for x in negative2])\n",
    "assert X2.shape[0] == y2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_validate2, y_train2, y_validate2 = sklearn.model_selection.train_test_split(\n",
    "    X2, y2, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "clf2, cvres2 = xgb_cross_validate(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98601398601398604"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(X_validate2, y_validate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 0.0% - FRR: 100.0%'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far2, frr2 = far_frr(clf2, X_validate2, y_validate2)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with this defintion of the positive / negative examples, we get at a higher FAR and FRR. It is especially noteworthy to look at the higher FAR: this could indicate that the classifier takes more into account how a subject thinks (i.e. a certain style of EEG over time) than what a subject thinks (i.e. the tasks a subject thinks about: unlabeled, breathe, song)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these results might be good. \n",
    "\n",
    "But our classifier's accuracy could be misleading.   \n",
    "\n",
    "Why? \n",
    "\n",
    "# Nonstationarity\n",
    "\n",
    "We are training, and testing, using data recorded over a single session. However, EEG changes over time, a property known as *nonstationarity*. Will our great results still hold a few weeks later?\n",
    "\n",
    "Let's take subject `A`'s data from sessions 1 and 2, which were recorded a few weeks after session 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_subja_sess1 = readings_right_subject_right_task('A', 'unlabeled', 1)\n",
    "X_subja_sess2 = readings_right_subject_right_task('A', 'unlabeled', 2)\n",
    "X_subja_later = np.concatenate([X_subja_sess1, X_subja_sess2])\n",
    "y_subja_later = [ 0 for x in X_subja_later ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the classifier we trained on the original data, testing it on the later data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 0.0% - FRR: 100.0%'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far, frr = far_frr(clf, X_subja_later, y_subja_later)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonstationarity is a problem for us. After all, we can calibrate our target subject, but we then expect them to leave the lab and go use the device later on. If their state changes so much that they can no longer be authenticated, we can't very well claim our system is accurate!\n",
    "\n",
    "So let's quantify and qualify *what* is changing in EEG signals over time.\n",
    "\n",
    "We could:\n",
    "- Study subject `A`'s recordings over the three sessions provided here.\n",
    "- Study one subject's recordings over the course of a year.\n",
    "\n",
    "Some questions to spur investigation:\n",
    "\n",
    "- What features of readings cause a classifier that works on earlier recordings fail on later ones?\n",
    "- What features remain the same? Are there any?\n",
    "- What might be the source of these changing features? Changing placement in the EEG device? Changing properties of the brain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, we might be interested in passthoughts that change over time. In others, we might not. One possibility to discover what features of readings cause a classifier that works on earlier recordings fail on later ones, would be to bandpass to the different types of waves (alpha, beta, theta, gamma). However, this is not possible with the current data, since that is already bandpassed, including all of these types of waves. We could reverse engineer the to_power_spectrum function from lab one.\n",
    "\n",
    "Also, if we had time domain data, a more advanced project could view the brain as a nonlinear dynamic system. It could find out whether the EEG data follows either a normal distribution or rather a power law. Features such as these, i.e. seeing the brain as following chaos theory, could shed new lights on what properties are changing in EEG signals over time. This view would take the stance that the source that is changing EEG signals over time is internal to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different approach could be as follows: The features of a task might cause the features of readings to change over time. For instance, we might hypothesize that breathing is a very common task that we have done millions of times in our lives. Just breathing (and thinking about it) should not change (much) over time. Compare this to the task of looking at a face: there is this concept in neuroscience of a grandmother neuron (or the Halle Berry neuron), which activates when a person sees his grandmother. Now something similar might exist (maybe in form of a pattern, and not a single cell) for every new task, such as, in this case: for every new face. Furthermore, we might argue that it needs time for this pattern (or single cell) to \"lock in\" - analogous to a muscle that needs to be accustomed to a new movement: in the beginning, you try out different movements until you converge to the perfect movement. \n",
    "So, the newness of a task might cause the features of readings to change over time.\n",
    "\n",
    "Let us look at the different tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 0: ['unlabeled' 'breathe' 'song' 'song_o' 'sport' 'breathe_o' 'speech' 'face']\n",
      "Session 1: ['unlabeled' 'calibration' 'word_x' 'phrase_x' 'face_x' 'breatheopen'\n",
      " 'song_x' 'sport_x']\n",
      "Session 2: ['unlabeled' 'calibration' 'breatheclosed' 'word_x' 'word_c' 'phrase_x'\n",
      " 'phrase_c' 'face_x' 'face_c' 'breatheopen' 'song_x' 'song_c' 'sport_x'\n",
      " 'sport_c']\n"
     ]
    }
   ],
   "source": [
    "print('Session 0:', dataset[(dataset['subject']=='A') & (dataset['session'] == 0)]['label'].unique())\n",
    "print('Session 1:', dataset[(dataset['subject']=='A') & (dataset['session'] == 1)]['label'].unique())\n",
    "print('Session 2:', dataset[(dataset['subject']=='A') & (dataset['session'] == 2)]['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Surprise, surprise, there are breathing tasks. Let us try the above analysis with the one of the breathing tasks then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23, 516), (23, 516))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive3 = readings_right_subject_right_task('A', 'breathe_o', 0)\n",
    "negative3 = readings_wrong_subj_any_task('A')\n",
    "positive3.shape, positive3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3 = np.concatenate([positive3, negative3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y3 = np.array([ 0 for x in positive3] + [ 1 for x in negative3])\n",
    "assert X3.shape[0] == y3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train3, X_validate3, y_train3, y_validate3 = sklearn.model_selection.train_test_split(\n",
    "    X3, y3, \n",
    "    test_size=0.33, \n",
    "    random_state=42)\n",
    "clf3, cvres3 = xgb_cross_validate(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99273607748184023"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.score(X_validate3, y_validate3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 0.0% - FRR: 100.0%'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far3, frr3 = far_frr(clf3, X_validate3, y_validate3)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# non-stationarity\n",
    "X_subja_sess1 = readings_right_subject_right_task('A', 'breatheopen', 1)\n",
    "X_subja_sess2 = readings_right_subject_right_task('A', 'breatheopen', 2)\n",
    "X_subja_later2 = np.concatenate([X_subja_sess1, X_subja_sess2])\n",
    "y_subja_later2 = [ 0 for x in X_subja_later ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FAR: 0.0% - FRR: 100.0%'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far3, frr3 = far_frr(clf3, X_subja_later2, y_subja_later2)\n",
    "f'FAR: {far*100}% - FRR: {frr*100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data so far fits my theory. There seems to be a very good classifier for the \"breatheopen\" task, as far as non-stationarity and FAR is concerned. Compared to other subjects, it classifies with a very low FAR and across time, for the same subject, it also classifies with a very low FAR.\n",
    "However, we might want to check for other tasks, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are of course, other possible sources of changing features:\n",
    "Newness and adapting brain patterns can be integrated under the concept of brain plasticity. The brain's neurons rewire constantly (from shorter time frames, as can be seen in conditioning experiments) to longer time frames, such as learning a very complex skill. There might be too few repetitions of the tasks for brain or synaptic plasticity to take place in a manner that is significant to change EEG signals. But we would expect a stronger effect over a longer period.\n",
    "\n",
    "Another possible source of changing features might actually also be the placement of the EEG device, but not necessarily: If we used multiple electrodes, the classifier should have enough information to be able to filter out the differences in placement (at least most of it). However, using only one electrode on a different place might dramatically change the signal. Just think about a blink of an eye recorded by an electrode on FP1 vs. that same blink recorded by an electrode on P3 - that would be a huge difference. So, in more general terms, the placement of the electrodes should affect the change of the features more, the less electrodes we use.\n",
    "\n",
    "And then, there are so many possible other factors: e.g. stress, which affects neuroplasticity, thus EEG signal changes. And so on for many other smaller factors to be taken into account. This view contrasts with the view of nonlinear dynamic systems in that we would view the source that is changing EEG signals over time as external to the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would have also wanted to visualize it to do the above data analysis not just for the breathe task, but eyeball potential other tasks that might look promising for new insights, or that might support or serve as counterexamples to my theory. Now, that will be a task for future endeavours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index(inplace=True)\n",
    "\n",
    "# Plot the raw_fft with standard error\n",
    "sns.tsplot(data=dataset, time=\"index\", unit=\"subject\",\n",
    "           condition=\"label\", value=\"raw_fft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
